{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "\n",
    "import typing\n",
    "from typing import Any, Tuple\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example data:\n",
      "Deutsch:  Ohne Zweifel findet sich auf dieser Welt zu jedem Mann genau die richtige Ehefrau und umgekehrt; wenn man jedoch in Betracht zieht, dass ein Mensch nur Gelegenheit hat, mit ein paar hundert anderen bekannt zu sein, von denen ihm nur ein Dutzend oder weniger nahesteht, darunter höchstens ein oder zwei Freunde, dann erahnt man eingedenk der Millionen Einwohner dieser Welt leicht, dass seit Erschaffung ebenderselben wohl noch nie der richtige Mann der richtigen Frau begegnet ist.\n",
      "English:  Doubtless there exists in this world precisely the right woman for any given man to marry and vice versa; but when you consider that a human being has the opportunity of being acquainted with only a few hundred people, and out of the few hundred that there are but a dozen or less whom he knows intimately, and out of the dozen, one or two friends at most, it will easily be seen, when we remember the number of millions who inhabit this world, that probably, since the earth was created, the right man has never yet met the right woman.\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "# Data from http://www.manythings.org/anki/.\n",
    "path_to_file = pathlib.Path(\"/tf/code/piplinjen/notebooks/deu-eng/deu.txt\")\n",
    "\n",
    "def load_data(path):\n",
    "    text = path.read_text(encoding='utf-8')\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    pairs = [line.split('\\t') for line in lines]\n",
    "    inp = [inp for targ, inp, _ in pairs]\n",
    "    targ = [targ for targ, inp, _ in pairs]\n",
    "\n",
    "    return targ, inp\n",
    "\n",
    "targ, inp = load_data(path_to_file)\n",
    "\n",
    "print(\"Example data:\")\n",
    "print(\"Deutsch: \", inp[-1])\n",
    "print(\"English: \", targ[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  tf.Tensor(\n",
      "[b'Lesen Sie diese Anweisungen.'\n",
      " b'Tom hat immer seine Unschuld bekr\\xc3\\xa4ftigt und behauptet bis heute, dass er verleumdet wurde.'\n",
      " b'Tom ist fettleibig.'\n",
      " b'Tom nahm mir das Versprechen ab, dort nicht wieder hinzugehen.'\n",
      " b'Ich lernte eifrig, als ich noch zur Schule ging.'], shape=(5,), dtype=string)\n",
      "Target:  tf.Tensor(\n",
      "[b'Read these instructions.'\n",
      " b'Tom has always maintained that he is innocent, and claims to this day that he was framed.'\n",
      " b'Tom is obese.' b'Tom made me promise not to go there again.'\n",
      " b'I studied hard when I was in school.'], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# Create a tensorflow dataset.\n",
    "BUFFER_SIZE = len(inp)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inp, targ)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "\n",
    "for inp_batch, targ_batch in dataset.take(1):\n",
    "    print(\"Input: \", inp_batch[:5])\n",
    "    print(\"Target: \", targ_batch[:5])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Verkr\\xc3\\xbcmele dich! Mach \\xe2\\x80\\x99ne Fliege! Mir ist hei\\xc3\\x9f.'\n",
      "b'Verkru\\xcc\\x88mele dich! Mach \\xe2\\x80\\x99ne Fliege! Mir ist hei\\xc3\\x9f.'\n",
      "Verkrümele dich! Mach ’ne Fliege! Mir ist heiß.\n",
      "[START] verkrumele dich !  mach ne fliege !  mir ist heiss . [END]\n"
     ]
    }
   ],
   "source": [
    "example_text = tf.constant(\"Verkrümele dich! Mach ’ne Fliege! Mir ist heiß.\")\n",
    "\n",
    "print(example_text.numpy())\n",
    "print(tf_text.normalize_utf8(example_text, 'NFKD').numpy())\n",
    "\n",
    "def tf_lower_and_split_punct(text):\n",
    "    # Split accecented characters,\n",
    "    text = tf_text.normalize_utf8(text, \"NFKD\")\n",
    "    text = tf.strings.lower(text)\n",
    "    # Replace special characters.\n",
    "    text = tf.strings.regex_replace(text, 'ß', 'ss')\n",
    "    # Keep space, a to z, and select punctuation.\n",
    "    text = tf.strings.regex_replace(text, '[^ a-z.?!,]', '')\n",
    "    # Add spaces around punctuation.\n",
    "    text = tf.strings.regex_replace(text, '[.?!,]', r' \\0 ')\n",
    "    # Strip whitespace.\n",
    "    text = tf.strings.strip(text)\n",
    "\n",
    "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "    return text\n",
    "\n",
    "print(example_text.numpy().decode())\n",
    "print(tf_lower_and_split_punct(example_text).numpy().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text vectorization.\n",
    "\n",
    "max_vocab_size = 5000\n",
    "\n",
    "input_text_processor = tf.keras.layers.TextVectorization(standardize=tf_lower_and_split_punct,\n",
    "max_tokens=max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '[START]', '[END]', '.', ',', 'ich', 'tom', '?', 'nicht']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text_processor.adapt(inp)\n",
    "\n",
    "# First 10 words from the vocabulary.\n",
    "input_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '[START]', '[END]', '.', 'tom', 'to', 'you', 'the', 'i']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text_processor = tf.keras.layers.TextVectorization(standardize=tf_lower_and_split_punct, max_tokens=max_vocab_size)\n",
    "\n",
    "output_text_processor.adapt(targ)\n",
    "output_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[   2  310   13   97 2464    4    3    0    0    0    0    0    0    0\n",
      "    0    0    0    0], shape=(18,), dtype=int64)\n",
      "[START] lesen sie diese anweisungen . [END]           \n"
     ]
    }
   ],
   "source": [
    "# These layers can convert a batch of strings into a batch of token IDs that are zero-padded.\n",
    "example_tokens = input_text_processor(inp_batch)\n",
    "print(example_tokens[0])\n",
    "input_vocab = np.array(input_text_processor.get_vocabulary())\n",
    "tokens = input_vocab[example_tokens[0].numpy()]\n",
    "print(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful class that enforces the right tensor dimensions.\n",
    "class ShapeChecker():\n",
    "    def __init__(self):\n",
    "        # Keep a cache of every axis-name seen.\n",
    "        self.shapes = {}\n",
    "\n",
    "    def __call__(self, tensor, names, broadcast=False):\n",
    "        if not tf.executing_eagerly():\n",
    "            return\n",
    "        \n",
    "        if isinstance(names, str):\n",
    "            names = (names,)\n",
    "\n",
    "        shape = tf.shape(tensor)\n",
    "        rank = tf.rank(tensor)\n",
    "\n",
    "        if rank != len(names):\n",
    "            raise ValueError(f\"Rank mismatch:\\n\"\n",
    "                             f\"     found {rank}: {shape.numpy()}\\n\"\n",
    "                             f\"     expected {len(names)}: {names}\\n\")\n",
    "\n",
    "        for i, name in enumerate(names):\n",
    "            if isinstance(name, int):\n",
    "                old_dim = name\n",
    "            else:\n",
    "                old_dim = self.shapes.get(name, None)\n",
    "            new_dim = shape[i]\n",
    "\n",
    "            if (broadcast and new_dim == 1):\n",
    "                continue\n",
    "\n",
    "            if old_dim is None:\n",
    "                # If the axis name is new, add its length to the cache.\n",
    "                self.shapes[name] = new_dim\n",
    "                continue\n",
    "\n",
    "            if new_dim != old_dim:\n",
    "                raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
    "                                f\"     found: {new_dim}\\n\"\n",
    "                                f\"     expected: {old_dim}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "\n",
    "        # The embedding layer converts tokens to vectors.\n",
    "        self.embedding = tf.keras.layers.Embedding(self.input_vocab_size, embedding_dim)\n",
    "\n",
    "        # The GRU RNN layer processes those vectors sequentially.\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "        return_sequences=True,\n",
    "        return_state=True,\n",
    "        recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, tokens, state=None):\n",
    "        shape_checker = ShapeChecker() \n",
    "        shape_checker(tokens, ('batch', 's'))\n",
    "\n",
    "        # Look up the embedding for each token.\n",
    "        vectors = self.embedding(tokens)\n",
    "        shape_checker(vectors, ('batch', 's', 'embed_dim'))\n",
    "\n",
    "        # The GRU processes the embedding sequence.\n",
    "        #   ouput shape: (batch, s, enc_units)\n",
    "        #   state shape: (batch, enc_units)\n",
    "        output, state = self.gru(vectors, initial_state=state)\n",
    "        shape_checker(output, ('batch', 's', 'enc_units'))\n",
    "        shape_checker(state, ('batch', 'enc_units'))\n",
    "\n",
    "        return output, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape: (batch): (64,)\n",
      "Input batch tokens shape: (batch, s): (64, 18)\n",
      "Encoder output shape: (batch, s, units): (64, 18, 1024)\n",
      "Encoder state shape: (batch, units): (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the encoder.\n",
    "\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "\n",
    "example_tokens = input_text_processor(inp_batch)\n",
    "\n",
    "encoder = Encoder(input_text_processor.vocabulary_size(),\n",
    "embedding_dim, units)\n",
    "example_enc_output, example_enc_state = encoder(example_tokens)\n",
    "\n",
    "print(f'Input batch shape: (batch): {inp_batch.shape}')\n",
    "print(f'Input batch tokens shape: (batch, s): {example_tokens.shape}')\n",
    "print(f'Encoder output shape: (batch, s, units): {example_enc_output.shape}')\n",
    "print(f'Encoder state shape: (batch, units): {example_enc_state.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention layer first calculates the attention weights, \n",
    "$$\\alpha_{ts} = \\frac{\\exp(\\text{score}(\\bf{h}_t, \\bar{h}_s))}{\\sum_{s'}\\exp(\\text{score}(\\bf{h}_t, \\bar{h}_{s'}))},$$\n",
    "where the score is $\\bold{v}_a^T \\text{tanh}(\\bold{W}_1 \\bold{h}_t + \\bold{W}_2\\bold{\\bar{h}}_s)$.\n",
    "And then the context vector,\n",
    "$$\\bold{c}_t = \\sum_s \\alpha_{ts}\\bar{\\bold{h}}_s.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "        self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "\n",
    "        self.attention = tf.keras.layers.AdditiveAttention()\n",
    "\n",
    "    def call(self, query, value, mask):\n",
    "        \"\"\"\n",
    "        The query is generated by the decoder.\n",
    "        The value is the output of the encoder.\n",
    "        The mask is to exclude padding.\n",
    "        \"\"\"\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(query, ('batch', 't', 'query_units'))\n",
    "        shape_checker(value, ('batch', 's', 'value_units'))\n",
    "        shape_checker(mask, ('batch', 's'))\n",
    "\n",
    "\n",
    "        w1_query = self.W1(query)\n",
    "        shape_checker(w1_query, ('batch', 't', 'attn_units'))\n",
    "        w2_key = self.W2(value)\n",
    "        shape_checker(w2_key, ('batch', 's', 'attn_units'))\n",
    "\n",
    "        query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
    "        value_mask = mask\n",
    "\n",
    "        context_vector, attention_weights = self.attention(inputs = [w1_query, value, w2_key], mask=[query_mask, value_mask], return_attention_scores = True)\n",
    "        shape_checker(context_vector, ('batch', 't', 'value_units'))\n",
    "        shape_checker(attention_weights, ('batch', 't', 's'))\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 18])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test of attention layer.\n",
    "attention_layer = BahdanauAttention(units)\n",
    "(example_tokens != 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch_size, query_seq_length, units):  (64, 2, 1024)\n",
      "Attention weights shape: (batch_size, query_seq_length, value_seq_length):  (64, 2, 18)\n"
     ]
    }
   ],
   "source": [
    "# The decoder will generate this.\n",
    "example_attention_query = tf.random.normal(shape=[len(example_tokens), 2, 10])\n",
    "\n",
    "context_vector, attention_weights = attention_layer(\n",
    "    query=example_attention_query,\n",
    "    value = example_enc_output,\n",
    "    mask=(example_tokens != 0)\n",
    ")\n",
    "\n",
    "print(f\"Attention result shape: (batch_size, query_seq_length, units):  {context_vector.shape}\")\n",
    "print(f\"Attention weights shape: (batch_size, query_seq_length, value_seq_length):  {attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The decoder will generate predictions for the next output token.\n",
    "#   - The output of the encoder is used as input for the decoder.\n",
    "#   - The RNN layer keeps track of what has been generated so far.\n",
    "#   - The RNN output serves as the query to the attention over the encoder's output to produce the context vector.\n",
    "#   - It combines the RNN output and the context vector to generate the attention vector.\n",
    "#       attention vector, a_t = f(c_t, h_t) = tanh(W_c[c_t;h_t])\n",
    "#   - It generates logit predictions for the next token based on the attention vector.\n",
    "\n",
    "class DecoderInput(typing.NamedTuple):\n",
    "    new_tokens: Any\n",
    "    enc_output: Any\n",
    "    mask: Any\n",
    "\n",
    "class DecoderOutput(typing.NamedTuple):\n",
    "    logits: Any\n",
    "    attention_weights: Any \n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(self.output_vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units, \n",
    "                    return_sequences=True, \n",
    "                    return_state=True, \n",
    "                    recurrent_initializer='glorot_uniform')\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "        # To create the attention vector.\n",
    "        self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh, use_bias=False)\n",
    "        # To create logit predictions.\n",
    "        self.fc = tf.keras.layers.Dense(self.output_vocab_size)\n",
    "\n",
    "    def call(self, inputs: DecoderInput, state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(inputs.new_tokens, ('batch', 't'))\n",
    "        shape_checker(inputs.enc_output, ('batch', 's', 'enc_units'))\n",
    "        shape_checker(inputs.mask, ('batch', 's'))\n",
    "\n",
    "        if state is not None:\n",
    "            shape_checker(state, ('batch', 'dec_units'))\n",
    "\n",
    "        vectors = self.embedding(inputs.new_tokens)\n",
    "        shape_checker(vectors, ('batch', 't', 'embedding_dim'))\n",
    "\n",
    "        # Process one step with the RNN.\n",
    "        rnn_output, state = self.gru(vectors, initial_state=state)\n",
    "        shape_checker(rnn_output, ('batch', 't', 'dec_units'))\n",
    "        shape_checker(state, ('batch', 'dec_units'))\n",
    "\n",
    "        # Use the RNN output as the query for the attention over the encoder output.\n",
    "        context_vector, attention_weights = self.attention(\n",
    "            query=rnn_output, \n",
    "            value=inputs.enc_output, \n",
    "            mask=inputs.mask\n",
    "        )\n",
    "        shape_checker(context_vector, ('batch', 't', 'dec_units'))\n",
    "        shape_checker(attention_weights, ('batch', 't', 's'))\n",
    "\n",
    "        context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
    "        attention_vector = self.Wc(context_and_rnn_output)\n",
    "        shape_checker(attention_vector, ('batch', 't', 'dec_units'))\n",
    "\n",
    "        logits = self.fc(attention_vector)\n",
    "        shape_checker(logits, ('batch', 't', 'output_vocab_size'))\n",
    "\n",
    "        return DecoderOutput(logits, attention_weights), state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape: (batch_size, t, output_vocab_size) (64, 1, 5000)\n",
      "state shape: (batch_size, dec_units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(output_text_processor.vocabulary_size(), embedding_dim, units)\n",
    "\n",
    "example_output_tokens = output_text_processor(targ_batch)\n",
    "\n",
    "start_index = output_text_processor.get_vocabulary().index('[START]')\n",
    "first_token = tf.constant([[start_index]] * example_output_tokens.shape[0])\n",
    "\n",
    "dec_result, dec_state = decoder(\n",
    "    inputs = DecoderInput(\n",
    "        new_tokens=first_token,\n",
    "        enc_output=example_enc_output,\n",
    "        mask=(example_tokens != 0)\n",
    "        ),\n",
    "    state = example_enc_state\n",
    ")\n",
    "\n",
    "print(f\"logits shape: (batch_size, t, output_vocab_size) {dec_result.logits.shape}\")\n",
    "print(f\"state shape: (batch_size, dec_units) {dec_state.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
